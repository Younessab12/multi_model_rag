{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.llms import Ollama\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain import hub\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOpenAI(\n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         temperature=0,\n",
    "#         api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "#     )\n",
    "model = 'llama3.1'\n",
    "llamaURL = \"http://localhost:11434\"\n",
    "llm = Ollama(model=model, base_url=llamaURL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_model = OpenAIEmbeddings(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "embedding_model = OllamaEmbeddings(model=model, base_url=llamaURL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyperparameters are configurations set before training a machine learning model. They are not learned during training but are crucial in defining the model's performance and training process. Hyperparameters can be categorized into two main types:\n",
      "\n",
      "1. Model Hyperparameters:\n",
      "   - Define the architecture and complexity of the model.\n",
      "   - Examples include:\n",
      "     * Number of layers in a neural network\n",
      "     * Number of neurons in each layer\n",
      "     * Type of activation functions (e.g., ReLU, Sigmoid)\n",
      "     * Kernel size in convolutional layers\n",
      "\n",
      "2. Training Hyperparameters:\n",
      "   - Control the learning process and optimization of the model.\n",
      "   - Examples include:\n",
      "     * Learning rate: Determines the step size for weight updates.\n",
      "     * Batch size: Number of training examples processed at a time.\n",
      "     * Number of epochs: Total iterations over the entire training dataset.\n",
      "     * Optimizer type: Choice of algorithm for optimization (e.g., SGD, Adam).\n",
      "     * Dropout rate: Percentage of neurons deactivated to prevent overfitting.\n",
      "\n",
      "Hyperparameter tuning involves methods such as grid search, random search, or automated tools like Optuna and Hyperopt to find optimal configurations. Selecting appropriate hyperparameters can improve accuracy, efficiency, and generalization.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Text = \"\"\"\n",
    "Hyperparameters are configurations set before training a machine learning model. They are not learned during training but are crucial in defining the model's performance and training process. Hyperparameters can be categorized into two main types:\n",
    "\n",
    "1. Model Hyperparameters:\n",
    "   - Define the architecture and complexity of the model.\n",
    "   - Examples include:\n",
    "     * Number of layers in a neural network\n",
    "     * Number of neurons in each layer\n",
    "     * Type of activation functions (e.g., ReLU, Sigmoid)\n",
    "     * Kernel size in convolutional layers\n",
    "\n",
    "2. Training Hyperparameters:\n",
    "   - Control the learning process and optimization of the model.\n",
    "   - Examples include:\n",
    "     * Learning rate: Determines the step size for weight updates.\n",
    "     * Batch size: Number of training examples processed at a time.\n",
    "     * Number of epochs: Total iterations over the entire training dataset.\n",
    "     * Optimizer type: Choice of algorithm for optimization (e.g., SGD, Adam).\n",
    "     * Dropout rate: Percentage of neurons deactivated to prevent overfitting.\n",
    "\n",
    "Hyperparameter tuning involves methods such as grid search, random search, or automated tools like Optuna and Hyperopt to find optimal configurations. Selecting appropriate hyperparameters can improve accuracy, efficiency, and generalization.\n",
    "\"\"\"\n",
    "print(Text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512,chunk_overlap=128)\n",
    "chunks = text_splitter.split_text(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma.from_texts(chunks, embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = create_retrieval_chain(combine_docs_chain=llm,retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell Latitude 5420\\Desktop\\Projects\\RAG\\rag_env\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'input'] optional_variables=['chat_history'] input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x00000220D8E618A0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]} partial_variables={'chat_history': []} metadata={'lc_hub_owner': 'langchain-ai', 'lc_hub_repo': 'retrieval-qa-chat', 'lc_hub_commit_hash': 'b60afb6297176b022244feb83066e10ecadcda7b90423654c4a9d45e7a73cebc'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='Answer any use questions based solely on the context below:\\n\\n<context>\\n{context}\\n</context>'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "print(retrieval_qa_chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_docs_chain = create_stuff_documents_chain(llm,retrieval_qa_chat_prompt)\n",
    "# print(combine_docs_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = create_retrieval_chain(retriever,combine_docs_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "qst = \"what are hyperparameters?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = retrieval_chain.invoke({\"input\":qst})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters are configurations set before training a machine learning model. They're not learned during training but are crucial in defining the model's performance and training process.\n"
     ]
    }
   ],
   "source": [
    "print(res['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_path='db'\n",
    "# db = Chroma(\n",
    "#     persist_directory=db_path,embedding_function=embedding_model\n",
    "# )\n",
    "# results = db.similarity_search_with_score(qst,k=10)\n",
    "# context = \"\\n\\n----\\n\\n\".join([doc.page_content for doc,_score in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT_TEMPLATE =\"\"\"\n",
    "# Aswer the question based only on the following context:\n",
    "# {context}\n",
    "\n",
    "# ----\n",
    "# answer the question based on the above context: {question}\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "\n",
    "# prompt = prompt_template.format(context=context,question=qst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\H'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\H'\n",
      "C:\\Users\\Dell Latitude 5420\\AppData\\Local\\Temp\\ipykernel_10452\\2275492172.py:4: SyntaxWarning: invalid escape sequence '\\H'\n",
      "  \"data\\Hyperparameters_in_ML.pdf\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_paths = [\n",
    "  \"data\\Hyperparameters_in_ML.pdf\"\n",
    "]\n",
    "\n",
    "documents = []\n",
    "\n",
    "for pdf_path in pdf_paths:\n",
    "  loader = PyPDFLoader(pdf_path)\n",
    "  documents.extend(loader.load())\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_spliter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "context = text_spliter.split_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = 'rag_env/db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell Latitude 5420\\AppData\\Local\\Temp\\ipykernel_10452\\43129953.py:7: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "# if os.path.exists(db_path):\n",
    "#     shutil.rmtree(db_path)\n",
    "db = Chroma(\n",
    "    persist_directory=db_path,embedding_function=embedding_model\n",
    ")\n",
    "db.add_documents(context)\n",
    "db.persist()\n",
    "print(\"added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
